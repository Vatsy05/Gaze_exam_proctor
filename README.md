# Exam Gaze Proctor ğŸ‘€

**Exam Gaze Proctor** is an AI-powered system designed to monitor students during online exams.  
It uses **eye gaze tracking**, **face detection**, and **audio monitoring** to flag potentially suspicious behaviors such as looking away from the screen, keeping eyes closed for too long, the presence of multiple people, or background conversations.  

Built with **TensorFlow, OpenCV, MediaPipe, and SoundDevice**, the project demonstrates how deep learning and computer vision can be applied to real-world proctoring scenarios.

---

## âœ¨ Key Features

### ğŸ”¹ Eye Gaze Tracking
- Classifies gaze into three categories:  
  - **Center** â†’ Looking directly at the screen.  
  - **Away** â†’ Looking left, right, up, or down.  
  - **Closed** â†’ Eyes shut.  
- Powered by a custom-trained CNN on both an open dataset and personal calibration images.  
- Uses **temporal smoothing** and **confidence thresholds** to reduce false positives.

---

### ğŸ”¹ Closed-Eyes Timeout
- Detects if the userâ€™s eyes remain closed for an extended duration.  
- **If eyes are closed for >5 seconds:**  
  - Red warning overlay is displayed.  
  - Event is logged in `events/events.log`.  
  - A short MP4 clip is saved as evidence.  
- Helps differentiate between natural blinking and suspicious inactivity.

---

### ğŸ”¹ Multi-Face Detection
- Ensures only one face is present in the camera feed.  
- If **more than one face** is detected:  
  - Immediate red flash warning (*MULTIPLE FACES DETECTED!*).  
  - Event is logged with a timestamp.  
  - Video evidence is recorded automatically.  
- Useful to prevent collaboration or proxy test-taking.

---

### ğŸ”¹ Audio Monitoring
- Continuously listens through the microphone for **speech or background voices**.  
- If **sustained voice activity** is detected:  
  - Red warning overlay is displayed (*VOICE DETECTED!*).  
  - Event is logged in `events/events.log`.  
  - A short `.wav` audio clip (~5 seconds) is saved as evidence.  
- Prevents students from discussing answers out loud or receiving verbal help.

---

### ğŸ”¹ Real-Time Warnings
- During suspicious activity, the studentâ€™s screen flashes **red** with a bold warning message:  
  - *LOOKING AWAY!*  
  - *MULTIPLE FACES DETECTED!*  
  - *EYES CLOSED TOO LONG!*  
  - *VOICE DETECTED!*  
- Provides **instant feedback** to discourage further misconduct.  

---

### ğŸ”¹ Evidence Logging
- Every suspicious event is logged in the `events/` folder.  
- **Two types of evidence are saved:**  
  1. A line in `events.log` with the timestamp and event type.  
  2. A short **.mp4 video clip** or **.wav audio clip** of the violation.  
- Ensures instructors can review exactly what happened later.

---

### ğŸ”¹ Robust Classification
- **Closed â‰  Away** â†’ closed eyes are treated as a separate class.  
- Combines:
  - **Short-term checks** (catch quick closed-eye signals).  
  - **Ratio-based checks** (e.g., away â‰¥60% of last frames).  
  - **Long-term smoothing** (majority vote across ~25 frames).  
- Results in a system that is stable against natural blinks, quick glances, or small posture changes.

---

## ğŸ“‚ Project Structure

```
Exam_Proctor/
â”œâ”€â”€ scripts/                # Dataset capture & preprocessing scripts
â”œâ”€â”€ raw_images/             # Personal + open dataset images
â”œâ”€â”€ processed/              # Preprocessed eye crops ready for training
â”œâ”€â”€ model_train.py          # CNN training script
â”œâ”€â”€ infer_realtime.py       # Real-time proctoring system
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ events/                 # Logs + MP4 evidence of suspicious events
```

---

## ğŸ”„ System Pipeline

```mermaid
flowchart TD
    A[Webcam Feed] --> B(MediaPipe Face Mesh)
    B --> C(Eye Crop + Preprocessing)
    C --> D(CNN Gaze Classifier)
    D -->|Center| E1[Normal State]
    D -->|Away| E2[Flag Away]
    D -->|Closed| E3[Closed Eye Timeout Check]

    B --> F(Face Detection - Multi-Face)
    F -->|More than 1 Face| E4[Flag Multiple Faces]

    H[Microphone Audio] --> I(Audio Monitor)
    I -->|Voice Detected| E5[Flag Audio Activity]

    E2 --> G[Red Flash + Log Event]
    E3 --> G
    E4 --> G
    E5 --> G

    G --> H2[Save MP4/WAV Evidence + events.log]
```
---

## ğŸ“Š Current Capabilities

âœ… Eye gaze classification (center / away / closed)

âœ… Temporal smoothing for stable predictions

âœ… Closed-eyes timeout detection (>5s)

âœ… Multi-face detection with warnings

âœ… Audio monitoring for background voices

âœ… Red flash overlay warnings for violations

âœ… Evidence logging (event log + MP4/WAV clips)

---

## ğŸ”® Planned Improvements

â¬œ Dataset Expansion â†’ integrate more open datasets to improve CNN generalization

â¬œ Cheat Behavior Scenarios â†’ add detection for mobile phone usage, frequent head tilting, etc.

â¬œ Instructor Dashboard â†’ centralized log and evidence review system.

â¬œ Optional Cloud Sync â†’ store violations securely for remote review. 

---

ğŸ‘¨â€ğŸ’» Author

Developed by Vathsal Upadhyay (Vatsy05)
ğŸ’¡ Built as a college-level AI/ML project to showcase practical application of CNNs, MediaPipe, and real-time proctoring.